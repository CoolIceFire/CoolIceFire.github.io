---
layout:     post
title:      "Random Forest"
subtitle:   "Random Forest Learning"
date:       2016-07-06
author:     "LiGuang"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
---

> “Yeah It's on. ”


## 前言
随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定，本文介绍一下随机森林，并且在[kaggle(Titanic)](https://www.kaggle.com/c/titanic)上进行实验。

<p id = "build"></p>
---

## 正文
相信大家都了解决策树，随机森林是由许多的决策树组成。决策树有许多的优点，比如训练速度很快、模型简单等，但是在决策树中很容易出现过拟合(over-fitting)等。而随机森林则在很大程度上避免了过拟合现象的出现，随机森林中每棵决策树之间是没有联系的，测试样本在随机森林中进行判断，根据这些决策树输出类别的众树来决定预测样本属于哪一类。

随机森林的生成规则：

1. 训练集大小为N，对于随机森林中每棵决策树的建立，随机并且有放回的从训练集中抽取出N个样本，然后利用这N个样本建立一棵决策树(如果不是随机抽样的话，那么随机森林中所有决策树的训练集都是相同的，那和一棵决策树也就没有任何区别了，毫无意义)
2. 训练集的特征数为M，从中选择m<<M个特征
3. 建立决策树，建树过程中无剪枝

影响随机森林分类准确率的因素：

1. 森林中任意两棵树的相关性：相关性越大，错误率越高
2. 森林中每棵树的分类能力：能力越强，错误率越低
3. 减小特征选择数m，树的相关性和分类能力就相应的降低，反之则增大，所以选择一个合适的m使随机森林的分类能力最优是关键问题。

随机森林中决策树的建立有多种方法：ID3、C4.5等，它们有不同的决策树划分选择如信息增益、增益率、基尼系数等。

对于kaggle上的Titanic问题：

1. 有的值如Sex、Pclass等是离散属性值，而有的值如Age、Fare等属于连续值，需要用不同的方法处理。
2. 缺失值处理：在训练集中有些数据是缺失的，如有些乘客的Age、Fare等，在建树时需要对缺失值进行处理，我用的方法是直接取的平均值。
3. 选择可以用来建树的特征：在此之中，我认为Name、Ticket、Cablin对Survived/Died没有影响，所以选择剩余的作为建树特征。
4. 将数据id化，此处将Sex、Embarked进行id化。

OK，开始建树测试……[代码](https://github.com/CoolIceFire/RS/blob/master/random_forest_titanic_kaggle.py)在这，数据可以在kaggle上下载。

## 后记
准确率挺低的，才79%，觉得还需要修改修改。

## 参考
