---
layout:     post
title:      "卷积神经网络(CNN)"
subtitle:   "卷积神经网络学习笔记一"
date:       2016-06-07
author:     "LiGuang"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 算法
---

> “Yeah It's on. ”


## 前言
卷积神经网络（Convolutional Neural Network, CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更优的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要估计的参数更少，使之成为一种颇具吸引力的深度学习结构。

这篇博文算是自己的CNN学习笔记，自己学的也不透彻，错误的地方，欢迎指正。

<p id = "build"></p>
---

## 局部连接(Sparse Connectivity)
在神经网络中，连接存在全连接与局部连接两种方式。下图是全连接与局部连接两种连接方式的对比：
![](https://raw.githubusercontent.com/CoolIceFire/CoolIceFire.github.io/master/img/20160607/01.jpg)
左图为全连接，右图为局部连接。途中Image大小为1000x1000，隐藏节点为10^6个。采取全连接方式时，图片中的每个节点与每个隐藏节点都有连接，所以需要参数的个数为1000x1000x10^6=10^12个，右图中采取局部连接的方式，每个隐藏单元只与大小为10x10的局部图像相连，参数的个数为10x10x10^6=10^9，一下子减少了3个数量级！

## 权值共享
在上文中，通过局部连接的方式虽然减少了参数的个数，但是参数的数量仍然非常的多，如何解决呢？通过权值共享的方式就很好的解决了这个问题。在局部连接中，每个隐藏节点的权值参数为10x10，将一个隐藏节点的参数共享给所有的隐藏节点，也就是说所有的10^6个隐藏节点的权值是相同的。那么，现在参数的个数就一下子降为了100个！这10x10个权值参数也就是卷积核的大小。但是，一个卷积核只能提取到图像的一种特征，如果想要提取图像的多种特征，那么就需要多个卷积核，从而得到多种特征图(Feature Map)。有多少个卷积核，就能得到多少个特征图。在卷积网络中，不仅仅权值是共享的，偏置也是共享的。下图中描述的非常清楚：
![](https://raw.githubusercontent.com/CoolIceFire/CoolIceFire.github.io/master/img/20160607/02.jpg)
## 网络结构
下图是经典的CNN结构，即LeNet-5网络。
![](https://raw.githubusercontent.com/CoolIceFire/CoolIceFire.github.io/master/img/20160607/03.png)
INPUT:大小为32x32的图片

C1：卷积层，用6个大小为5x5的卷积核通过卷积运算得到6个大小为28x28的卷积特征图

S2：下采样层，用大小为2x2的池化窗口得到大小为14x14的图像

C3：卷积层，用16个大小为5x5的卷积核通过卷积运算得到10个大小为10x10的卷积特征图

S4：下采样层，用大小为2x2的池化窗口得到5x5的图像

C5：卷积层，通过全连接得到120个1x1的特征图

F6：通过全连接得到84个1x1单元

OUTPUT:采用欧式径向基函数单元

下面将详细讲解一下每层的原理。

**C1**

C1是卷积层，那么如何进行卷积运算呢？利用卷积核在输入中不断的进行滑动，通过将输入值与卷积核中权值参数进行运算得到，即将卷积核中的各个参数与相对应的局部像素值相乘加和，然后再加上偏置参数得到。用[UFLDL](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution)中图能够很生动的描述这一过程：
![](https://raw.githubusercontent.com/CoolIceFire/CoolIceFire.github.io/master/img/20160607/04.gif)

在图中可以看到，输入大小为5x5，卷积核大小为3x3，随着卷积核在输入层中不断的滑动，将卷积核中各个参数与对应的输入层的值相乘加和得到了卷积运算之后的值，那么卷积特征图的大小为(5-3+1)x(5-3+1)=9，那么一共需要多少个参数呢？一共需要3x3=9个。通常情况下卷积层参数个数为(卷积核大小+1)x卷积核个数。其中的1指的是偏置。在LeNet-5网络中，用6个大小为5x5的卷积核经过卷积运算即可得到C1层(包括6个特征图，每个特征图大小为(32-5+1)^2=28x28)。

**S2**

在通过卷积获得到特征之后，我们希望利用这些特征去做分类，但是如果直接去分类，此时有每个样例都会得到一个28x28x6=4704维卷积特征向量，很容易出现过拟合。为了解决这个问题，对不同位置的特征进行聚合统计，通过计算图像一个区域上的某个特定特征的平均值(或最大值)，可以得到低维度、高精度的统计特征，这种聚合方法就叫做的池化(pooling)。

下图是一个池化的过程(来自UFLDL)：
![](https://raw.githubusercontent.com/CoolIceFire/CoolIceFire.github.io/master/img/20160607/05.gif)
S2是一个下采样层，池化窗口的大小为2x2，所以S2中特征图的大小是C1的1/4。如何进行计算呢？与卷积层类似，将每个单元的4个值进行相加，然后乘以权值，再加上偏置，结果通过sigmoid运算即可得到。由于权值共享，所以一共需要训练的参数的个数为(1+1)x6=12个。

**C3/S4与上面所讲类似**

**C5**

C5层也是卷积层，不过这一层是全连接而不是局部连接，120个神经元，每个单元与S4层全部特征图相连接，每个单元的参数个数为5x5x16+1=401个，一共120个单元，所以总共为401x120=48120个参数需要训练。

**F6**

这一层也是全连接，计算方法也用输入向量和权重向量之间的点积，再加上一个偏置，然后将其传递给sigmoid函数产生单元i的一个状态。一共有(120+1)x84=10164个参数需要训练。

**OUTPUT**

输出层由欧式径向基函数单元组成，每类一个单元，每个有84个输入，也就是每个输出单元计算输入向量和参数向量之间的欧式距离，输入离参数向量越远，其输出也就越大。输出可以被理解为F6层配置空间的高斯分布的**-log-likelihoood**

##后记
以上是简要介绍，后续有CNN的训练推导以及用CNN进行数字识别的应用介绍。

不正之处，欢迎指出！
